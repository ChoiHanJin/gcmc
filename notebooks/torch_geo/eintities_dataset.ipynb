{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Entities dataset based on PyTorch geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import gzip\n",
    "# rdfはWeb上のリンクをグラフ構造で表してWeb自体を\n",
    "# 体系的な知識構造にしようと言うところででてきたやつ\n",
    "import rdflib as rdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "from torch_geometric.data import (InMemoryDataset, Data, \n",
    "                                  download_url, extract_tar)\n",
    "from torch_geometric.utils import one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entities(InMemoryDataset):\n",
    "    url = 'https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/{}.tgz'\n",
    "    \n",
    "    def __init__(self, root, name, transform=None, pre_transform=None):\n",
    "        assert name in ['AIFB', 'AM', 'MUTAG', 'BGS']\n",
    "        self.name = name.lower()\n",
    "        super(Entities, self).__init__(root, transform, pre_transform)\n",
    "        # processed_path[0]は処理された後のデータで，process methodで定義される\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    \"\"\"\n",
    "    ここで定義されている@propertyはclass_name.propertyとして\n",
    "    アクセスできるようにしているデコレータで，計算してからself.~~と\n",
    "    今まで書いていたものを，このようにスマートに定義することが可能らしい．いいね．\n",
    "    知らなかったのは多分やばい．．．\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def num_relations(self):\n",
    "        return self.data.edge_type.max().item() + 1\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return self.data.train_y.max().item() + 1\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [\n",
    "            '{}_stripped.nt.gz'.format(self.name),\n",
    "            'completeDataset.tsv',\n",
    "            'trainingSet.tsv',\n",
    "            'testSet.tsv'\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'data.pt'\n",
    "    \n",
    "    \n",
    "    def download(self):\n",
    "        path = download_url(self.url.format(self.name), self.root)\n",
    "        extract_tar(path, self.raw_dir)\n",
    "        os.unlink(path)\n",
    "        \n",
    "    def triples(self, graph, relation=None):\n",
    "        for s, p, o in graph.triples((None, relation, None)):\n",
    "            yield s, p, o\n",
    "            \n",
    "    def process(self):\n",
    "        graph_file, task_file, train_file, test_file = self.raw_paths\n",
    "        \n",
    "        g = rdf.Graph()\n",
    "        with gzip.open(graph_file, 'rg') as f:\n",
    "            g.parse(file=f, format='nt')\n",
    "            \n",
    "        relations = sorted(set(g.predicates()), key=lambda rel: -freq(rel))\n",
    "        subjects = set(g.subjects())\n",
    "        objects = set(g.objects())\n",
    "        nodes = list(subjects.union(objects))\n",
    "        \n",
    "        relations_dict = {rel: i for i in enumerate(list(relations))}\n",
    "        nodes_dict = {node: i for i, node in enumerate(nodes)}\n",
    "        \n",
    "        edge_list = []\n",
    "        for s, p, o in g.triples((None, None, None)):\n",
    "            src, dst, rel = nodes_dict[s], nodes_dict[o], relations_dict[p]\n",
    "            edge_list.append([src, dst, 2 * rel])\n",
    "            edge_list.append([dst, src, 2 * rel + 1])\n",
    "            \n",
    "        edge_list = sorted(edge_list, key=lambda x: (x[0], x[1], x[2]))\n",
    "        edge = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "        edge_index, edge_type = edge[:2], edge[2]\n",
    "        \n",
    "        oh = one_hot(edge_type, 2 * len(relations), dtype=torch.float)\n",
    "        deg = scatter_add(oh, edge_index[0], dim=0, dim_size=len(nodes))\n",
    "        index = edge_type + torch.arange(len(edge_list)) * 2 * len(relations)\n",
    "        edge_norm = 1 / deg[edge_index[0]].view(-1)[index]\n",
    "        \n",
    "        if self.name == 'am':\n",
    "            label_header = 'label_category'\n",
    "            nodes_header = 'proxy'\n",
    "        elif self.name == 'aifb':\n",
    "            label_header = 'label_affiliation'\n",
    "            nodes_header = 'person'\n",
    "        elif self.name == 'mutag':\n",
    "            label_header = 'label_mutagenic'\n",
    "            nodes_header = 'bond'\n",
    "        elif self.name == 'bgs':\n",
    "            label_header = 'label_lithogenesis'\n",
    "            nodes_header = 'rock'\n",
    "            \n",
    "            \n",
    "        labels_df = pd.read_csv(task_file, sep='\\t')\n",
    "        labels_set = set(labels_df[label_header].values.tolist())\n",
    "        labels_dict = {lab: i for i, lab in enumerate(list(labels_set))}\n",
    "        nodes_dict = {np.unicode(key): val for key, val in nodes_dict.items()}\n",
    "        \n",
    "        train_labels_df = pd.read_csv(train_file, sep='\\t')\n",
    "        train_indices, train_lables = [], []\n",
    "        for nod, lab in zip(train_labels_df[nodes_header].values,\n",
    "                           train_labels_df[label_header].values):\n",
    "            train_indices.append(nodes_dict[nod])\n",
    "            train_labels.append(labels_dict[lab])\n",
    "            \n",
    "        train_idx = torch.tensor(train_indices, dtype=torch.long)\n",
    "        train_y = torch.tensor(train_labels, dtype=torch.long)\n",
    "        \n",
    "        test_labels_df = pd.read_csv(test_file, sep='\\t')\n",
    "        test_indices, test_labels = [], []\n",
    "        for nod, lab in zip(test_labels_df[nodes_header].values, \n",
    "                            test_labels_df[label_header].values):\n",
    "            test_indices.append(nodes_dict[nod])\n",
    "            test_labels.append(labels_dict[lab])\n",
    "            \n",
    "        test_idx = torch.tensor(test_indices, dytpe=torch.long)\n",
    "        test_y = torch.tensor(test_labels, dtype=torch.long)\n",
    "        \n",
    "        data = Data(edge_index=edge_index)\n",
    "        data.edge_type = edge_type\n",
    "        data.edge_norm = edge_norm\n",
    "        data.train_idx = train_idx\n",
    "        data.train_y = train_y\n",
    "        data.test_idx = test_idx\n",
    "        data.test_y = test_y\n",
    "        \n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '{}{}()'.format(self.name.upper(), self.__class__.__name__)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
